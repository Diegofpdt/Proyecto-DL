{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0afc40c3",
   "metadata": {},
   "source": [
    "Antes de correr este c√≥digo correr el script ¬¥Creacion_tokenizer_nuevo_modelo.py¬¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700a5b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MarianTokenizer, MarianMTModel, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf13900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Usando dispositivo: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\n",
    "\t\"mps\" if torch.backends.mps.is_available()\n",
    "\telse \"cuda\" if torch.cuda.is_available()\n",
    "\telse \"cpu\"\n",
    ")\n",
    "print(f\"üîß Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc53602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MarianTokenizer.from_pretrained(\"tokenizer_gallego_expandido\")\n",
    "model = MarianMTModel.from_pretrained(\"modelo_gallego_expandido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b585d795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens gallego: ['‚ñÅOl', 'a', '‚ñÅ', 'mun', 'do', '‚ñÅ', ',', 'como', 'est√°', '‚ñÅ', 's', '?', '</s>']\n"
     ]
    }
   ],
   "source": [
    "sample_gl_text = \"Ola mundo, como est√°s?\"\n",
    "tokenized_sample = tokenizer(sample_gl_text, return_tensors=\"pt\")\n",
    "print(f\"Tokens gallego: {tokenizer.convert_ids_to_tokens(tokenized_sample['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f34c1e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Par√°metros entrenables: 243537088\n",
      "üìä Par√°metros LoRA: 8486080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:550: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,  # Aumentado para mejor capacidad\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\",  # Attention layers\n",
    "        \"fc1\", \"fc2\"  # Feed-forward layers\n",
    "               # Embedding layers - CR√çTICOS para nuevos tokens\n",
    "        \"embed_tokens\",  # Input embeddings - donde est√°n los nuevos tokens\n",
    "        \n",
    "        # Output projection - NECESARIO para generar tokens gallegos\n",
    "        \"lm_head\"  # Final projection to vocabulary\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(f\"üìä Par√°metros entrenables: {model.num_parameters()}\")\n",
    "print(f\"üìä Par√°metros LoRA: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da0b7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset cargado: 997 pares de frases\n"
     ]
    }
   ],
   "source": [
    "ds_en = load_dataset(\"openlanguagedata/flores_plus\", \"eng_Latn\", split=\"dev\")\n",
    "ds_gl = load_dataset(\"openlanguagedata/flores_plus\", \"glg_Latn\", split=\"dev\")\n",
    "\n",
    "# Crear dataset paralelo\n",
    "parallel_data = []\n",
    "for e, g in zip(ds_en, ds_gl):\n",
    "    if len(e[\"text\"].strip()) > 0 and len(g[\"text\"].strip()) > 0:\n",
    "        parallel_data.append({\"en\": e[\"text\"], \"gl\": g[\"text\"]})\n",
    "\n",
    "print(f\"‚úÖ Dataset cargado: {len(parallel_data)} pares de frases\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33478aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(parallel_data)\n",
    "\n",
    "# Dividir en train/validation\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00971327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Preprocesando datos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 897/897 [00:01<00:00, 888.52 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 1013.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Procesar en lotes\n",
    "    sources = examples[\"en\"]\n",
    "    targets = examples[\"gl\"]\n",
    "    \n",
    "    # Tokenizar fuentes y objetivos en una sola llamada\n",
    "    model_inputs = tokenizer(\n",
    "        sources,\n",
    "        text_target=targets,  # Nuevo m√©todo recomendado\n",
    "        max_length=128, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Reemplazar padding tokens en labels con -100\n",
    "    model_inputs[\"labels\"] = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
    "        for label in model_inputs[\"labels\"]\n",
    "    ]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Aplicar preprocesamiento\n",
    "print(\"üîÑ Preprocesando datos...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "777e2fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/wy/2w2n4vp14556bvkvcx_j87w00000gn/T/ipykernel_82921/1111489380.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=None,  # Desactivar wandb/tensorboard\n",
    "    dataloader_pin_memory=False,  # Para MPS\n",
    ")\n",
    "\n",
    "# 8. Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# 9. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a85feef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3375' max='3375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3375/3375 12:26, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.473800</td>\n",
       "      <td>4.071459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.601900</td>\n",
       "      <td>3.548279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.327000</td>\n",
       "      <td>3.375678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.120400</td>\n",
       "      <td>3.315549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.885300</td>\n",
       "      <td>3.262609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.859600</td>\n",
       "      <td>3.217035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.749200</td>\n",
       "      <td>3.187695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.675500</td>\n",
       "      <td>3.160486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.682500</td>\n",
       "      <td>3.154196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.621300</td>\n",
       "      <td>3.146463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.551500</td>\n",
       "      <td>3.143080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.463100</td>\n",
       "      <td>3.135019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.460300</td>\n",
       "      <td>3.142805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.489000</td>\n",
       "      <td>3.131944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.437200</td>\n",
       "      <td>3.132207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.401700</td>\n",
       "      <td>3.133783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Entrenamiento completado!\n",
      "üíæ Guardando modelo...\n",
      "üîó Fusionando y guardando modelo final...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:392: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications. You can opt to merge the adapter after cloning the weights (to untie the embeddings). You can untie the embeddings by loading the model with `tie_word_embeddings=False`. For example:\n",
      "```python\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "# Load original tied model\n",
      "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", tie_word_embeddings=False)\n",
      "\n",
      "# Set the randomly initialized lm_head to the previously tied embeddings\n",
      "model.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n",
      "\n",
      "# Save the untied model\n",
      "untied_model_dir = \"dir/for/untied/model\"\n",
      "model.save_pretrained(untied_model_dir)\n",
      "model.config.save_pretrained(untied_model_dir)\n",
      "\n",
      "# Now use the original model but in untied format\n",
      "model = AutoModelForCausalLM.from_pretrained(untied_model_dir)\n",
      "```\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo guardado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "print(\"‚úÖ Entrenamiento completado!\")\n",
    "\n",
    "# Guardar modelo\n",
    "print(\"üíæ Guardando modelo...\")\n",
    "trainer.save_model(\"opus-en-gl-lora\")\n",
    "tokenizer.save_pretrained(\"opus-en-gl-lora\")\n",
    "\n",
    "# Guardar modelo fusionado\n",
    "print(\"üîó Fusionando y guardando modelo final...\")\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(\"opus-en-gl-lora-fused\")\n",
    "tokenizer.save_pretrained(\"opus-en-gl-lora-fused\")\n",
    "\n",
    "print(\"‚úÖ Modelo guardado exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b45d8e",
   "metadata": {},
   "source": [
    "Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c3c71b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e3fa76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translation_batch(sentences_en, references_gl, model_path=\"opus-en-gl-lora-fused\"):\n",
    "    try:\n",
    "        test_model = MarianMTModel.from_pretrained(model_path)\n",
    "        test_model.eval()\n",
    "        test_model.to(device)\n",
    "\n",
    "        test_tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        inputs = test_tokenizer(sentences_en, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = test_model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        translations = [test_tokenizer.decode(out, skip_special_tokens=True) for out in outputs]\n",
    "\n",
    "        for src, pred, ref in zip(sentences_en, translations, references_gl):\n",
    "            print(f\"EN: {src}\")\n",
    "            print(f\"GL (pred): {pred}\")\n",
    "            print(f\"GL (ref):  {ref}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        # Calcular m√©tricas BLEU y chrF en lote\n",
    "        print(\"\\nüìä M√©tricas globales:\")\n",
    "        bleu = load(\"bleu\")\n",
    "        chrf = load(\"chrf\")\n",
    "\n",
    "        bleu_score = bleu.compute(predictions=translations, references=[[r] for r in references_gl])\n",
    "        chrf_score = chrf.compute(predictions=translations, references=references_gl)\n",
    "\n",
    "        print(f\"BLEU: {bleu_score['bleu']:.4f}\")\n",
    "        print(f\"chrF: {chrf_score['score']:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en traducci√≥n: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e412140a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: \"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.\n",
      "GL (pred): \"Non mos camundongos de 4 me se s de camundongos que no n diab√©ticas que utilizan o diab√©tico\", frisou o diab√©tico.\n",
      "GL (ref):  \"Agora temos ratos de 4 meses que xa non son diab√©ticos, pero que no seu momento si que o foron\", engadiu.\n",
      "------------------------------------------------------------\n",
      "EN: Dr. Ehud Ur, professor of medicine at Dalhousie University in Halifax, Nova Scotia and chair of the clinical and scientific division of the Canadian Diabetes Association cautioned that the research is still in its early days.\n",
      "GL (pred): O Dr. Ehud Ur, professor de me dicine da Universi da de de Dalhousie en Halifax, Nova Sc√≥cia, e presidente da divisi√≥n cl√≠nica e cientifica de que a cl√≠nica de diabete de Ca na dian acauterou que o arco de re se nto se est√° no m√°is de diabete.\n",
      "GL (ref):  O Dr. Ehud Ur, profesor de medicina na Universidade Dalhousie en Halifax (Nova Escocia) e presidente da divisi√≥n cl√≠nica e cient√≠fica da Asociaci√≥n canadense contra a diabetes advertiu de que a investigaci√≥n a√≠nda est√° nunha fase inicial.\n",
      "------------------------------------------------------------\n",
      "EN: Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no relevance to people who already have Type 1 diabetes.\n",
      "GL (pred): Como outros experts, √© c√©tico sobre a certeza de que o diabete pode se curar, no n n que os casos de diabete de tipo 1 no r e van ciam que o diabete diabete de tipo 1.\n",
      "GL (ref):  Do mesmo xeito que outros expertos, am√≥sase esc√©ptico de que a diabetes te√±a cura e apunta que estes achados non son relevantes para as persoas que xa te√±en diabetes de tipo 1.\n",
      "------------------------------------------------------------\n",
      "EN: On Monday, Sara Danius, permanent secretary of the Nobel Committee for Literature at the Swedish Academy, publicly announced during a radio program on Sveriges Radio in Sweden the committee, unable to reach Bob Dylan directly about winning the 2016 Nobel Prize in Literature, had abandoned its efforts to reach him.\n",
      "GL (pred): En Mon da y, Sara Danius, se cretario permanente do Comit√© Noble de Literatura da Acad√©mi da de Sueca, un no acci√≥n p√∫blica dun programa de radio na radio na Sveriges de Sweden, o comit√©, utiliza do para alcancar Bob Dylan de forma directa a gaar o Premio Noble de Literatura de 2016, baniu os ten esfor dos para alcang√°-lo.\n",
      "GL (ref):  O pasado luns, Sara Danius, a secretaria permanente do Comit√© do Nobel de Literatura da Academia Sueca, anunciou en p√∫blico durante unha entrevista no programa de radio da cadea Sveriges Radio en Suecia que o comit√©, ao non lograr contactar directamente con Bob Dylan acerca do outorgamento do Premio Nobel de Literatura 2016, decidira abandonar os intentos de falar con el.\n",
      "------------------------------------------------------------\n",
      "EN: Danius said, \"Right now we are doing nothing. I have called and sent emails to his closest collaborator and received very friendly replies. For now, that is certainly enough.\"\n",
      "GL (pred): Danius adiantou: ¬´No rte, no n faremos n algo. Convoco e se n e-mails ao rtor de cla se s colla bo e recep√≠mos respostas a bo nte amiza das . Para no r, isto √© e no ugh¬ª.\n",
      "GL (ref):  Danius afirmou: ¬´Agora mesmo non estamos a facer nada. Chamei e envieille correos electr√≥nicos ao seu colaborador m√°is pr√≥ximo e recib√≠n respostas moi amables. De momento, con iso abonda¬ª.\n",
      "------------------------------------------------------------\n",
      "EN: Previously, Ring's CEO, Jamie Siminoff, remarked the company started when his doorbell wasn't audible from his shop in his garage.\n",
      "GL (pred): Antes, o CEO de Ring Ring, Jamie Simi no ff, re mar cou a que a empresa iniciou a bo ba do que o audible do r do r do ring no n era aud√≠vel desde a loj√≥n na garza.\n",
      "GL (ref):  Previamente, o director executivo de Ring, Jamie Siminoff, destacou que a empresa naceu cando non puido o√≠r o timbre da porta desde o seu taller no garaxe.\n",
      "------------------------------------------------------------\n",
      "EN: He built a WiFi door bell, he said.\n",
      "GL (pred): Wi-fi wifi wifi wifi wifi, disse.\n",
      "GL (ref):  Afirmou que constru√≠u un timbre wifi.\n",
      "------------------------------------------------------------\n",
      "EN: Siminoff said sales boosted after his 2013 appearance in a Shark Tank episode where the show panel declined funding the startup.\n",
      "GL (pred): Sharp√≠n Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro Xaro xaro xaro xaro xaro xar\n",
      "GL (ref):  Siminoff afirmou que as vendas se incrementaron despois da s√∫a aparici√≥n en 2013 no episodio Shark Tank, no que o panel do programa rexeitou financiar a empresa emerxente.\n",
      "------------------------------------------------------------\n",
      "EN: In late 2017, Siminoff appeared on shopping television channel QVC.\n",
      "GL (pred): No fi na nto de 2017, Simi no ff actuou no c entre s de evisi√≥n de CVV.\n",
      "GL (ref):  Foi a finais de 2017 cando Siminoff apareceu na canle de compras por televisi√≥n QVC.\n",
      "------------------------------------------------------------\n",
      "EN: Ring also settled a lawsuit with competing security company, the ADT Corporation.\n",
      "GL (pred): Ring Ring ring r ben un acquisi√≥n com a empresa de se curi√≥n de a casa la me nto, a ADT Corporation.\n",
      "GL (ref):  Ring tam√©n chegou a un acordo extraxudicial cunha empresa de seguridade competidora, a ADT Corporation.\n",
      "------------------------------------------------------------\n",
      "EN: While one experimental vaccine appears able to reduce Ebola mortality, up until now, no drugs have been clearly demonstrated suitable for treating existing infection.\n",
      "GL (pred): A pesar de que un experi me ntal vaci na parece poder reducir a morbi da de de E bo la, un diago, no n se demostrou ni que as drogas se ntam aptas para tratar a infecci√≥n de infecci√≥n de infecci√≥n de infecci√≥n de invenci√≥n de infecci√≥n de infecci√≥n de invenci√≥n de infecci√≥n de infecci√≥n de invenci√≥n de invenci√≥n.\n",
      "GL (ref):  A√≠nda que os indicios apuntan a que unha vacina experimental √© capaz de reducir a mortalidade causada polo √©bola, ata o momento, ning√∫n medicamento demostrou de xeito evidente a s√∫a capacidade para tratar esa infecci√≥n.\n",
      "------------------------------------------------------------\n",
      "EN: One antibody cocktail, ZMapp, initially showed promise in the field, but formal studies indicated it had less benefit than sought in preventing death.\n",
      "GL (pred): Un drinque anti bo cial, ZMapp, mostrou a promi se in the fi el d, pero os extro me nto indicou ter me nte ben do r do que busca do en previ na r a morte.\n",
      "GL (ref):  Un c√≥ctel de anticorpos, o ZMapp, presentou nun principio resultados prometedores nese campo, pero outros estudos formais apuntaron a que presentaba menos beneficios dos esperados na prevenci√≥n da mortalidade.\n",
      "------------------------------------------------------------\n",
      "EN: In the PALM trial, ZMapp served as a control, meaning scientists used it as a baseline and compared the three other treatments to it.\n",
      "GL (pred): No teste de PALMA, ZMapp se rved como control, os s√≠nte se usan se como un campo de LAM e compararan os tres outros trs de LAM.\n",
      "GL (ref):  No xu√≠zo contra PALM, ZMapp actuou como control, √© dicir, os cient√≠ficos empreg√°rono como li√±a base e compararon tres outros medicamentos con el.\n",
      "------------------------------------------------------------\n",
      "EN: USA Gymnastics supports the United States Olympic Committee's letter and accepts the absolute need of the Olympic family to promote a safe environment for all of our athletes.\n",
      "GL (pred): Os gyms de gyms de gyms amiza a carta do Comit√© Ol√≠mpico dos EE. UU. e aceita a absoluta necesi da da da da da da fam√≠la ol√≠mpica para promoir un environ me nto de gyms para to dos os gyms de gyms.\n",
      "GL (ref):  USA Gymnastics apoia a carta do Comit√© Ol√≠mpico dos EE. UU. e acepta a imperiosa necesidade de que a familia ol√≠mpica fomente unha contorna segura para todas as atletas.\n",
      "------------------------------------------------------------\n",
      "EN: We agree with the USOC's statement that the interests of our athletes and clubs, and their sport, may be better served by moving forward with meaningful change within our organization, rather than decertification.\n",
      "GL (pred): Concord√°mos com o esta do do do U.S.O.C. que os interestos dos d el atletas e clubs, e o se sports, poden se r pre se r se r se a van cando con a me nte a modificaci√≥n dentro da nos , en vez de se rtificaci√≥n de ratificaci√≥n.\n",
      "GL (ref):  Concordamos coa declaraci√≥n do USOC que afirma que os intereses dos nosos atletas e clubs, e do seu deporte, poder√≠an estar mellor atendidos pensando no futuro cun cambio significativo dentro na nosa organizaci√≥n, en lugar da retirada da certificaci√≥n.\n",
      "------------------------------------------------------------\n",
      "EN: USA Gymnastics supports an independent investigation that may shine light on how abuse of the proportion described so courageously by the survivors of Larry Nassar could have gone undetected for so long and embraces any necessary and appropriate changes.\n",
      "GL (pred): U.S. Gym nas tics d√° apoio a unha investigaci√≥n independente que pode iludir sobre a forma como a parte da probaci√≥n descri√≠ da de maneira tao valorosa por parte dos sobreviventes de Larry Nasar poderia ter se descoe do por tanto tempo e a do r as modificaci√≥ns necessarias e adequadas.\n",
      "GL (ref):  USA Gymnastics apoia unha investigaci√≥n independente que poder√≠a aclarar como uns abusos das dimensi√≥ns descritas con tanta coraxe polas superviventes de Larry Nassar puideron pasar desapercibidas durante tanto tempo e acolle todos os cambios necesarios e apropiados.\n",
      "------------------------------------------------------------\n",
      "EN: USA Gymnastics and the USOC have the same goal ‚Äî making the sport of gymnastics, and others, as safe as possible for athletes to follow their dreams in a safe, positive and empowered environment.\n",
      "GL (pred): U.S. Gym nas tics e o U.S. O U.S.O. tem o sa me goal (tor na r o sport of gym nas tics, e outros), de forma a que os gyms actuem de forma o me nte segura, positiva e enpoderosa.\n",
      "GL (ref):  O USA Gymnastics e o USOC te√±en o mesmo obxectivo: facer que a ximnasia como deporte, as√≠ como outros m√°is, sexan o m√°is seguros posible para que as atletas poidan seguir os seus so√±os nunha contorna segura, positiva e empoderada.\n",
      "------------------------------------------------------------\n",
      "EN: Throughout 1960s, Brzezinski worked for John F. Kennedy as his advisor and then the Lyndon B. Johnson administration.\n",
      "GL (pred): Ao longo de 60 s, Brzezinski cobrou a John F. Kennedy o cargo de con se r e, en seguida, a administraci√≥n de Lyndon B. John John son .\n",
      "GL (ref):  Foi ao longo da d√©cada dos 60 cando Brzezinski traballou para John F. Kennedy en calidade de asesor e, deseguido, para a administraci√≥n de Lyndon B. Johnson.\n",
      "------------------------------------------------------------\n",
      "EN: During the 1976 selections he advised Carter on foreign policy, then served as National Security Advisor (NSA) from 1977 to 1981, succeeding Henry Kissinger.\n",
      "GL (pred): Durante as se s lecci√≥ns de 1976, adviu a Carter sobre a pol√≠tica exte na e, xa, assumiu o cargo de Natio na l Security Advisor (NSA) de 1977 a 1981, suceder a Henry Kissinger Kissinger Kissinger.\n",
      "GL (ref):  Durante as elecci√≥ns de 1976, asesorou a Carter sobre pol√≠tica exterior e, posteriormente, traballou como Asesor de Seguridade Nacional (NSA) entre 1977 e 1981, como sucesor de Henry Kissinger.\n",
      "------------------------------------------------------------\n",
      "EN: As NSA, he assisted Carter in diplomatically handling world affairs, such as the Camp David Accords, 1978; normalizing US‚ÄìChina relations thought the late 1970s; the Iranian Revolution, which led to the Iran hostage crisis, 1979; and the Soviet invasion in Afghanistan, 1979.\n",
      "GL (pred): A.N.A., como A.N. A.N., a da ptou a Carter na manipulaci√≥n diplom√°tico dos problemas mundiais, como os Accords de Camp David, de 1978; as no r mal izaci√≥ns EUA-Chi na r ela ci√≥ns pensa das no fir me nto da A.N.A.; a Revoluci√≥n Iraniana que levou √° cri se de sofistos de Iran, de 1979; e a Inva sovi√©tica en Af√°n do A.\n",
      "GL (ref):  Na Axencia Nacional de Seguridade, asistiu a Carter na xesti√≥n diplom√°tica de temas internacionais, como os Acordos de Camp David, en 1978; a normalizaci√≥n das relaci√≥ns entre EE. UU. e China ata finais dos anos 70; a revoluci√≥n iraniana, que desencadeou a crise dos ref√©ns iranianos, en 1979; e a invasi√≥n sovi√©tica de Afganist√°n, en 1979.\n",
      "------------------------------------------------------------\n",
      "EN: The movie, featuring Ryan Gosling and Emma Stone, received nominations in all major categories.\n",
      "GL (pred): O rix ano r Rian Gosling e a rm √≠a r pia, no mi na ci√≥ns en to das as m√°is s de r√°n.\n",
      "GL (ref):  A pel√≠cula, cos actores Ryan Gosling e Emma Stone, foi nominada en todas as principais categor√≠as.\n",
      "------------------------------------------------------------\n",
      "EN: Gosling and Stone received nominations for Best Actor and Actress respectively.\n",
      "GL (pred): Gosling e Stone Actriz no n ten mi na ci√≥ns de Actor e Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz Actriz A Activa Actriz Actriz Actriz Actriz Actriz A Activa A Activa A Activa A Activa A\n",
      "GL (ref):  Gosling e Stone recibiron nominaci√≥ns a Mellor Actor e Actriz, respectivamente.\n",
      "------------------------------------------------------------\n",
      "EN: The other nominations include Best Picture, Director, Cinematography, Costume Design, Film-editing, Original Score, Production Design, Sound Editing, Sound Mixing and Original Screenplay.\n",
      "GL (pred): As outras no mi na ci√≥ns includes: Melhor Retrato, Director, Cinematograf √≠a , Costu me Design, Film editing, Orici na l Score, Producci√≥n de Design, Sound Editing, Sound Mixing e Orici na l Screen Screen Screen Screen Screenplay.\n",
      "GL (ref):  Outras nominaci√≥ns tam√©n inclu√≠ron Mellor pel√≠cula, Mellor director, Mellor cinematograf√≠a, Mellor dese√±o de vestiario, Mellor edici√≥n, Mellor banda sonora orixinal, Mellor dese√±o de produci√≥n, Mellor edici√≥n de son, Mellor mestura de son e Mellor gui√≥n orixinal.\n",
      "------------------------------------------------------------\n",
      "EN: Two songs from the movie, Audition (The Fools Who Dream) and City of Stars, received nominations for best original song. Lionsgate studio received 26 nominations ‚Äî more than any other studio.\n",
      "GL (pred): Dois son s do filme, Audition (The Fools Who Dream) e City of Stars, recepciron no mi na ci√≥ns para obter o se r son g. Lionsgate studio recepciou 26 no mi na ci√≥nsmais do que outro studio.\n",
      "GL (ref):  D√∫as canci√≥ns da pel√≠cula, Audition (The Fools Who Dream) e City of Stars, foron nominadas a mellor canci√≥n orixinal. O estudio Lionsgate recibiu 26 nominaci√≥ns: m√°is ca ning√∫n outro estudio.\n",
      "------------------------------------------------------------\n",
      "EN: Late on Sunday, the United States President Donald Trump, in a statement delivered via the press secretary, announced US troops would be leaving Syria.\n",
      "GL (pred): Late on Sun da y, o presidente dos EE. UU., Do na ld Trump, en un esta do me nto ataba do via o cretario de prensas, e no r que as tropas dos EE. UU. saiam de S√≠ria.\n",
      "GL (ref):  A √∫ltima hora do domingo, o presidente dos EE. UU., Donald Trump, anunciou a trav√©s dunha declaraci√≥n emitida polo secretario de prensa que as tropas estadounidenses √≠an abandonar Siria.\n",
      "------------------------------------------------------------\n",
      "EN: The announcement was made after Trump had a phone conversation with Turkish President Recep Tayyip Erdoƒüan.\n",
      "GL (pred): O aviso foi feito apoi do de que Trompete tivera unha conversi√≥n por telefone com o presidente Turco, Recep Tayyip Er do an.\n",
      "GL (ref):  Este anuncio realizouse despois de que Trump falase por tel√©fono co presidente turco Recep Tayyip Erdoƒüan.\n",
      "------------------------------------------------------------\n",
      "EN: Turkey would also take over guarding captured ISIS fighters which, the statement said, European nations have refused to repatriate.\n",
      "GL (pred): O Isis, o Isis, e o Isis, o Isis, capturaram o Isis, o que, o esta da do informou, as na ci√≥ns europeias refutou se para o Isis, o Isis.\n",
      "GL (ref):  Turqu√≠a tam√©n se encargar√≠a da custodia dos soldados do EIIL que, segundo a declaraci√≥n, os pa√≠ses europeos rexeitaron repatriar.\n",
      "------------------------------------------------------------\n",
      "EN: This not only confirms that at least some dinosaurs had feathers, a theory already widespread, but provides details fossils generally cannot, such as color and three-dimensional arrangement.\n",
      "GL (pred): Isto no n s√≥ confirma que ao me me me me nte os urs ten as penas, unha teor √≠a que se alarga r, pero os c√≥mios os f√≥sseis no n poden, como a cor e o c√≥mo do c√≥mo do .\n",
      "GL (ref):  Isto confirma que como m√≠nimo alg√∫ns dinosauros ti√±an plumas (unha teor√≠a xa estendida) e ademais proporciona outros datos que, polo xeral, os f√≥siles non nos ofrecen, como poden ser a cor e a disposici√≥n tridimensional.\n",
      "------------------------------------------------------------\n",
      "EN: . Scientists say this animal's plumage was chestnut-brown on top with a pale or carotenoid-colored underside.\n",
      "GL (pred): Os s ten da r que os s ten da s de que a pumaci√≥n deste ani mal era de castanho acasta do r no r do , con unha cla me nte ou de cor gor da do .\n",
      "GL (ref):  . Os cient√≠ficos afirman que as plumas do animal eran de cor casta√±a na parte superior e na parte inferior p√°lidas ou de cor carotenoide.\n",
      "------------------------------------------------------------\n",
      "EN: The find also grants insight into the evolution of feathers in birds.\n",
      "GL (pred): A do r a do r a introspecci√≥n sobre a evoluci√≥n de penas de penas en aves.\n",
      "GL (ref):  O achado tam√©n ofrece unha perspectiva sobre como evolucionaron as plumas nos paxaros.\n",
      "------------------------------------------------------------\n",
      "EN: Because the dinosaur feathers do not have a well-developed shaft, called a rachis, but do have other features of feathers ‚Äî barbs and barbules ‚Äî the researchers inferred the rachis was likely a later evolutionary development that these other features.\n",
      "GL (pred): Por que as pendentes de xeas no n ten unha coa xea , chamado rachis, no xeas xeas, no xeas outras carater√≠sticas de xeas (barbas e barbules) os re se archers inferiu que os rachis xaron √≠a se un dev me nto evolucio na rio r posterior que as outras carater√≠sticas.\n",
      "GL (ref):  Como as plumas dos dinosauros non te√±en un can√≥n ben desenvolvido, chamado raquis, pero si outras caracter√≠sticas propias das plumas (barbas e b√°rbulas), os investigadores deduciron que o raquis probablemente represente unha evoluci√≥n posterior a esas outras caracter√≠sticas.\n",
      "------------------------------------------------------------\n",
      "EN: The feathers' structure suggests that they were not used in flight but rather for temperature regulation or display. The researchers suggested that, even though this is the tail of a young dinosaur, the sample shows adult plumage and not a chick's down.\n",
      "GL (pred): A estrutura das pl √°s de pl √°s sugen que no n se usan o voo, pre se nto para regula me r da de da de da de da de da de r ou exibir da de da da de pl √°s . Os re se rx√≥ns sug√≠ran que, mesmo que esta √© a xea de un divisor moi da de r, amostra a puma da de r .\n",
      "GL (ref):  A estrutura das plumas suxire que non se usaban para voar, sen√≥n para regular a temperatura ou como enfeite. Os investigadores suxeriron que, a√≠nda que se trata da cola dun dinosauro novo, a mostra presenta plumaxe adulta e non se trata de penuxe dunha cr√≠a nova.\n",
      "------------------------------------------------------------\n",
      "EN: The researchers suggested that, even though this is the tail of a young dinosaur, the sample shows adult plumage and not a chick's down.\n",
      "GL (pred): Os arrastos de xeo sugeriu que, mesmo que esta √© a xea de un divisor de mo cidade , a exemplo mos aploraci√≥n de xeo de xeo de xeo de xeo de xeo de unha xeo de xeo de xeo.\n",
      "GL (ref):  Os investigadores suxeriron que, a√≠nda que se trata da cola dun dinosauro novo, a mostra presenta plumaxe adulta e non se trata de penuxe dunha cr√≠a nova.\n",
      "------------------------------------------------------------\n",
      "EN: A car bomb detonated at police headquarters in Gaziantep, Turkey yesterday morning killed two police officers and injured more than twenty other people.\n",
      "GL (pred): Unha car bo mb detonou na Sede de Policia en Gaziantep, Turqu√≠n, atax√≥n de ve do r, feriu do a do r do xe do r dos oficiais de Policia e lesi√≥n do feriu do a do r do r do r do xa do r do .\n",
      "GL (ref):  Un coche bomba detonado onte pola ma√±√° nunha comisar√≠a de polic√≠a en Gaziantep (Turqu√≠a) saldouse con dous axentes mortos e m√°is de vinte persoas feridas.\n",
      "------------------------------------------------------------\n",
      "EN: The governor's office said nineteen of the injured were police officers.\n",
      "GL (pred): O do r do r informou que dezenove dos lesi dos ten do s do s era un oficial de pol√≠cia.\n",
      "GL (ref):  Desde a oficina do gobernador afirmouse que dezanove feridos foron axentes da polic√≠a.\n",
      "------------------------------------------------------------\n",
      "EN: Police said they suspect an alleged Daesh (ISIL) militant of responsibility for the attack.\n",
      "GL (pred): A Polici da despi da de que suspeite un sui da do Daesh (E. IS) de responsabilidade do Isis.\n",
      "GL (ref):  A polic√≠a confirmou a sospeita de que un presunto militante do D√°esh (EIIL) foi o responsable do ataque.\n",
      "------------------------------------------------------------\n",
      "EN: They found the Sun operated on the same basic principles as other stars: The activity of all stars in the system was found to be driven by their luminosity, their rotation, and nothing else.\n",
      "GL (pred): Enxergou se que o Sol operava sobre os princ√≠pios basicos como as outras estrelas: A actividade de to das as estrelas no sistema foi acusa da por se r lumi nos , se r rotaci√≥n , e no r algo .\n",
      "GL (ref):  Descubriron que o Sol segue os mesmos principios b√°sicos que o resto das estrelas: comprobouse que a actividade de todas as estrelas no sistema se rexe pola s√∫a luminosidade, rotaci√≥n e nada m√°is.\n",
      "------------------------------------------------------------\n",
      "EN: The luminosity and rotation are used together to determine a star's Rossby number, which is related to plasma flow.\n",
      "GL (pred): Os lumi nos ita e a rotaci√≥n se usan para determinar o n√∫mero de rosbi na ci√≥n de estor na , o que resulta no fluxo de fluxo de plasma.\n",
      "GL (ref):  A luminosidade e a rotaci√≥n empr√©ganse conxuntamente para determinar un n√∫mero estelar Rossby, relacionado co fluxo de plasma.\n",
      "------------------------------------------------------------\n",
      "EN: The smaller the Rossby number, the less active the star with respect to magnetic reversals.\n",
      "GL (pred): O n√∫mero de Rossby √© me nos activo, o que √© a estrela face a invers√µes magn√©ticas.\n",
      "GL (ref):  Canto m√°is baixo sexa o n√∫mero Rossby, menor actividade ter√° a estrela en relaci√≥n √°s inversi√≥ns magn√©ticas.\n",
      "------------------------------------------------------------\n",
      "EN: During his trip, Iwasaki ran into trouble on many occasions.\n",
      "GL (pred): Durante a viaxe, Iwasaki causou problemas en diversas ocasi√≥ns.\n",
      "GL (ref):  Durante a s√∫a viaxe, Iwasaki meteuse en problemas en numerosas ocasi√≥ns.\n",
      "------------------------------------------------------------\n",
      "EN: He was robbed by pirates, attacked in Tibet by a rabid dog, escaped marriage in Nepal and was arrested in India.\n",
      "GL (pred): Roubou o furto por pirates, atacou o tibete por un rabi do g, espoliou o xerato de pirates en Tamanha e pre se nciou o piratear do √≠ndia.\n",
      "GL (ref):  Roub√°rono os piratas, un can rabioso atacouno no T√≠bet, escapou dunha voda en Nepal e foi arrestado na India.\n",
      "------------------------------------------------------------\n",
      "EN: The 802.11n standard operates on both the 2.4Ghz and 5.0Ghz frequencies.\n",
      "GL (pred): 802.11 no 802.11 no 802.11 no 802.11 no 802.11 .\n",
      "GL (ref):  A norma 802.11n funciona tanto coas frecuencias de 2,4¬†GHz e 5,0¬†GHz.\n",
      "------------------------------------------------------------\n",
      "EN: This will allow it to be backwards compatible with 802.11a, 802.11b and 802.11g, provided that the base station has dual radios.\n",
      "GL (pred): Isto permitir√° que o 802.11a, 802.11b e 802.11g con 802.11a e 802.11g, desde que a esta√ß√£o ba se dispa de r√°dios dun 802.11 .\n",
      "GL (ref):  Isto permitir√° unha compatibilidade retroactiva con 802.11a, 802.11b e 802.11g, sempre que esa estaci√≥n base te√±a radios duais.\n",
      "------------------------------------------------------------\n",
      "EN: The speeds of 802.11n are substantially faster than that of its predecessors with a maximum theoretical throughput of 600Mbit/s.\n",
      "GL (pred): 802.11n 802.11n √© substancialmente m√°is r√°xis do 802.11 no que se pre se nta, con 802.11 Mbit/s.\n",
      "GL (ref):  As velocidades de 802,11n son considerablemente m√°is r√°pidas que as dos seus predecesores cunha produci√≥n te√≥rica m√°xima de 600Mbit/s.\n",
      "------------------------------------------------------------\n",
      "EN: Duvall, who is married with two adult children, did not leave a big impression on Miller, to whom the story was related.\n",
      "GL (pred): Duvall, que √© mar qui do con teiros pupilos, no n deixou unha forte impresi√≥n sobre Miller, para a que a historia foi r ela da .\n",
      "GL (ref):  Duvall, casado e con dous fillos adultos, non deixou unha boa impresi√≥n en Miller, con quen estaba relacionada a historia.\n",
      "------------------------------------------------------------\n",
      "EN: When asked for comment, Miller said, \"Mike talks a lot during the hearing...I was getting ready so I wasn't really hearing what he was saying.\"\n",
      "GL (pred): Ao que me nte , Miller pediu a xeo de que ¬´Mikes falan ca da a au da de... estive a preparar o xeo para que o xeo desababa o xeo de que o xeo desabara¬ª.\n",
      "GL (ref):  Cando se lle pediu que opinase, Miller afirmou: ¬´Mike falou moito durante a vista‚Ä¶ Eu estaba prepar√°ndome, polo que non estiven realmente a escoitar o que dic√≠a¬ª.\n",
      "------------------------------------------------------------\n",
      "EN: \"We will endeavour to cut carbon dioxide emissions per unit of GDP by a notable margin by 2020 from the 2005 level,\" Hu said.\n",
      "GL (pred): \"Procur√°mo- nos para cortar as emiss√µes de CO2 por unidade do PIB por un mar gin no me nto de 2020 desde o rol de 2005.\", declarou Hu r.\n",
      "GL (ref):  \"Traballaremos para reducir as emisi√≥ns de di√≥xido de carbono por unidade de PIB nunha medida importante para o 2020 en comparaci√≥n co nivel de 2005\", afirmou Hu.\n",
      "------------------------------------------------------------\n",
      "EN: He did not set a figure for the cuts, saying they will be made based on China's economic output.\n",
      "GL (pred): No n ve n un n√∫mero para os xetos, que dirban que se far√° ba se r na output da output da output da s econ√≥micas de Chi na .\n",
      "GL (ref):  Non fixou unha cifra para os recortes e afirmou que se realizar√≠an segundo a produci√≥n econ√≥mica da China.\n",
      "------------------------------------------------------------\n",
      "EN: Hu encouraged developing countries \"to avoid the old path of polluting first and cleaning up later.\"\n",
      "GL (pred): Hu evitou que pous se a poluir pri me nte e pous√°r a po mar .\n",
      "GL (ref):  Hu animou aos pa√≠ses en v√≠as de desenvolvemento a que ¬´evitasen a vella ruta de contaminar primeiro e limpar despois¬ª.\n",
      "------------------------------------------------------------\n",
      "EN: He added that \"they should not, however, be asked to take on obligations that go beyond their development stage, responsibility and capabilities.\"\n",
      "GL (pred): Afirmou que ¬´ no n se , no n √© de se r que intervenham para as obrigaci√≥ns que superen o palco, a responsabili da de e a desponsi√≥n e a capacidade ¬ª.\n",
      "GL (ref):  Engadiu que ¬´non obstante, non se lles deb√≠a pedir que asumisen obrigas que fosen m√°is al√° da fase de desenvolvemento, responsabilidades e capacidades¬ª.\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìä M√©tricas globales:\n",
      "BLEU: 0.0822\n",
      "chrF: 36.63\n"
     ]
    }
   ],
   "source": [
    "ds_en = load_dataset(\"openlanguagedata/flores_plus\", \"eng_Latn\", split=\"devtest[:50]\")\n",
    "ds_gl = load_dataset(\"openlanguagedata/flores_plus\", \"glg_Latn\", split=\"devtest[:50]\")\n",
    "\n",
    "# Tomar una muestra de 50 ejemplos para visualizaci√≥n\n",
    "sample_en = ds_en[\"text\"][:50]\n",
    "sample_gl = ds_gl[\"text\"][:50]\n",
    "\n",
    "test_translation_batch(sample_en, sample_gl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4631608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/fieldbeat/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: Hello, how are you?\n",
      "GL (pred): H el lo, e tu?\n",
      "GL (ref):  Ola, como est√°s?\n",
      "------------------------------------------------------------\n",
      "EN: I love learning languages.\n",
      "GL (pred): Amo a aprendizagem de idiomas.\n",
      "GL (ref):  G√∫stame aprender idiomas.\n",
      "------------------------------------------------------------\n",
      "EN: The weather is beautiful today.\n",
      "GL (pred): O clima √© bel√≠ssimo para o dia.\n",
      "GL (ref):  O tempo √© fermoso hoxe.\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìä M√©tricas globales:\n",
      "BLEU: 0.0000\n",
      "chrF: 24.41\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I love learning languages.\",\n",
    "    \"The weather is beautiful today.\",\n",
    "]\n",
    "test_translation_batch(test_sentences, [\"Ola, como est√°s?\", \"G√∫stame aprender idiomas.\", \"O tempo √© fermoso hoxe.\"], model_path=\"opus-en-gl-lora-fused\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fieldbeat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
